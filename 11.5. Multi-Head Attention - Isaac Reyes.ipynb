{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c846f172",
   "metadata": {},
   "source": [
    "# 11.5. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedf0a7",
   "metadata": {},
   "source": [
    "## AUTOR: Isaac Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f89913",
   "metadata": {},
   "source": [
    "In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.\n",
    "\n",
    "To this end, instead of performing a single attention pooling, queries, keys, and values can be transformed with \n",
    " independently learned linear projections. Then these \n",
    " projected queries, keys, and values are fed into attention pooling in parallel. In the end, \n",
    " attention-pooling outputs are concatenated and transformed with another learned linear projection to produce the final output. This design is called multi-head attention,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf42068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias:\n",
    "use strict;\n",
    "use warnings;\n",
    "use Data::Dump qw(dump);\n",
    "use d2l;\n",
    "IPerl->load_plugin('Chart::Plotly');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fe9bb",
   "metadata": {},
   "source": [
    "### 11.5.1. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74bd64",
   "metadata": {},
   "source": [
    "Based on this design, each head may attend to different parts of the input. More sophisticated functions than the simple weighted average can be expressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2bd50",
   "metadata": {},
   "source": [
    "### 11.5.2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef088e",
   "metadata": {},
   "source": [
    "In our implementation, we choose the scaled dot product attention for each head of the multi-head attention. To avoid significant growth of computational cost and parametrization cost, we set \n",
    ". Note that \n",
    " heads can be computed in parallel if we set the number of outputs of linear transformations for the query, key, and value to \n",
    ". In the following implementation, \n",
    " is specified via the argument num_hiddens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8a1b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package MultiHeadAttention {\n",
    "    use base(\"d2l::Module\");\n",
    "#@Save\n",
    "    sub new {\n",
    "        my ($class, %args) = (\n",
    "            shift, \n",
    "            d2l->get_arguments(\n",
    "                num_hiddens => undef,\n",
    "                num_heads => undef,\n",
    "                dropout => undef,\n",
    "                use_bias => 0,\n",
    "                \\@_\n",
    "            )\n",
    "        );\n",
    "        my $self = $class->SUPER::new(%args);\n",
    "        $self->{num_heads} = $args{num_heads};\n",
    "        $self->{attention} = new d2l::DotProductAttention($args{dropout});\n",
    "        $self->{W_q} = mx->gluon->nn->Dense($args{num_hiddens}, use_bias => $args{use_bias}, flatten => 0);\n",
    "        $self->{W_k} = mx->gluon->nn->Dense($args{num_hiddens}, use_bias => $args{use_bias}, flatten => 0);\n",
    "        $self->{W_v} = mx->gluon->nn->Dense($args{num_hiddens}, use_bias => $args{use_bias}, flatten => 0);\n",
    "        $self->{W_o} = mx->gluon->nn->Dense($args{num_hiddens}, use_bias => $args{use_bias}, flatten => 0);\n",
    "        \n",
    "        map { $self->register_child($self->{$_}) } ('attention', 'W_q', 'W_k', 'W_v', 'W_o');\n",
    "        return bless($self, $class);\n",
    "    }\n",
    "\n",
    "    sub forward {\n",
    "        my ($self, $queries, $keys, $values, $valid_lens) = @_;\n",
    "        $queries = $self->transpose_qkv($self->{W_q}->forward($queries));\n",
    "        $keys = $self->transpose_qkv($self->{W_k}->forward($keys));\n",
    "        $values = $self->transpose_qkv($self->{W_v}->forward($values));\n",
    "        \n",
    "        if (defined $valid_lens) {\n",
    "            $valid_lens = $valid_lens->repeat($self->{num_heads}, axis => 0);\n",
    "        }\n",
    "        \n",
    "        my $output = $self->{attention}->forward($queries, $keys, $values, $valid_lens);\n",
    "        my $output_concat = $self->transpose_output($output);\n",
    "        return $self->{W_o}->forward($output_concat);\n",
    "    }\n",
    "\n",
    "    1;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a14268",
   "metadata": {},
   "source": [
    "To allow parallel computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f15f9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*MultiHeadAttention::transpose_output"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $transpose_qkv = sub {\n",
    "    my ($self, $X) = @_;\n",
    "    $X = $X->reshape([$X->shape->[0], $X->shape->[1], $self->{num_heads}, -1]);\n",
    "    $X = $X->transpose([0, 2, 1, 3]);\n",
    "    return $X->reshape([-1, $X->shape->[2], $X->shape->[3]]);\n",
    "};\n",
    "d2l->add_to_class('MultiHeadAttention', 'transpose_qkv', $transpose_qkv);\n",
    "\n",
    "my $transpose_output = sub {\n",
    "    my ($self, $X) = @_;\n",
    "    $X = $X->reshape([-1, $self->{num_heads}, $X->shape->[1], $X->shape->[2]]);\n",
    "    $X = $X->transpose([0, 2, 1, 3]);\n",
    "    return $X->reshape([$X->shape->[0], $X->shape->[1], -1]);\n",
    "};\n",
    "d2l->add_to_class('MultiHeadAttention', 'transpose_output', $transpose_output);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ef5d9",
   "metadata": {},
   "source": [
    "To test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1779c84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my ($num_hiddens, $num_heads) = (100, 5);\n",
    "my $attention = new MultiHeadAttention($num_hiddens, $num_heads, 0.5);\n",
    "$attention->initialize();\n",
    "my ($batch_size, $num_queries, $num_kvpairs) = (2, 4, 6);\n",
    "my $valid_lens = mx->nd->array([3, 2]);\n",
    "my $X = mx->nd->ones([$batch_size, $num_queries, $num_hiddens]);\n",
    "my $Y = mx->nd->ones([$batch_size, $num_kvpairs, $num_hiddens]);\n",
    "d2l->check_shape($attention->forward($X, $Y, $Y, $valid_lens), [$batch_size, $num_queries, $num_hiddens]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d8ddc",
   "metadata": {},
   "source": [
    "No hacer:\n",
    "#### 11.5.3. Summary\n",
    "Multi-head attention combines knowledge of the same attention pooling via different representation subspaces of queries, keys, and values. To compute multiple heads of multi-head attention in parallel, proper tensor manipulation is needed.\n",
    "\n",
    "##### 11.5.4. Exercises\n",
    "Visualize attention weights of multiple heads in this experiment.\n",
    "\n",
    "Suppose that we have a trained model based on multi-head attention and we want to prune less important attention heads to increase the prediction speed. How can we design experiments to measure the importance of an attention head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006a74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
